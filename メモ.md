
# summary

# models
## modelA(swin_...)
* CV
* img_size: xxx
* backbone
    * image:
    * text: bert (4層平均)
* lr: xxx...
* scheduler
* augmentation
* loss (ArcFace(s=, m=))
* optimizer
* batch_size
* linear_out

## modelB(...)
*
*

# distance
最終的には, euclidean distanceとcosine similarityを使った。

# Ensemble Method
4モデル * 3出力(concat, text, img) * 2距離の24票

# PostProcess
## 無理やり2件
* notebook: ->
## 単位あわせ
* notebook: ->

# validation
* groupKfold(5), group by label_group

# worked for us
* torch.cat(img_embeddings, text_embeddings) -> nn.Linear -> arcface
* output 3 embeddings (concat, image, text) [LB + 0.01]
* voting ensemble
* different learning rate for cnn/bert/fc

# not worked for us
* multimodal approach(image + text)
  * transformer
    * xxx
  * convolution
* ensemble
  * averaging cosine similarity
  * concatenate embedding -> calculate cosine similarity
* pretrain cnn and bert respectively
* bert
  * CLS layer
* cnn
  * efficientnet_b4, b5, b6, or larger models
  * gem(pooling)
* loss
  * AdaCos
  * SphereFace
  * TripletLoss
  * AdaptiveFace
* activation layer(Swish, GeLU, ...)
* large batch size(>=32)
* progressive learning
* tf-idf for ensemble